{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimising the Rosenbrock Function\n",
    "\n",
    "**Note**:  \n",
    "This notebook is just getting started! Sorry for any messy code and lack of math rigor. If you'd like to help improve it, please feel free to dive in!\n",
    "\n",
    "The Rosenbrock function is a classic non-convex optimization problem and a popular benchmark for testing optimization algorithms.\n",
    "\n",
    "### Rosenbrock Function Formula üìù\n",
    "The Rosenbrock function is defined as:\n",
    "\n",
    "$$\n",
    "f(x) = \\sum_{i=1}^{n-1} [100(x_{i+1} - x_i^2)^2 + (1 - x_i)^2]\n",
    "$$\n",
    "\n",
    "### Global Minimum üéØ\n",
    "For $N=3$, the global minimum is at $x = [1, 1, 1]$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms Exploration üöÄ\n",
    "\n",
    "So, here's my starting point: How do different optimization algorithms tackle this problem? Let's find out together!\n",
    "\n",
    "Let's kick things off with these algorithms:\n",
    "- **PyTorch**: Using its automatic differentiation and optimisation algorithms.\n",
    "- **Firedrake.adjoint**: Leveraging its automatic differentiation combined with SciPy's optimization algorithms.\n",
    "\n",
    "First up, we'll dive into `torch.optim`, a module packed with optimization algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 ¬µs, sys: 0 ns, total: 2 ¬µs\n",
      "Wall time: 5.01 ¬µs\n",
      "0 [0.674467, 0.508772, 0.303139] 0.396131\n",
      "1 [0.701306, 0.489729, 0.303139] 0.089660\n",
      "2 [0.701351, 0.492088, 0.303139] 0.089195\n",
      "3 [0.703105, 0.491983, 0.303139] 0.088710\n",
      "4 [0.702848, 0.493639, 0.303139] 0.088312\n",
      "5 [0.802620, 0.630874, 0.303139] 0.056713\n",
      "6 [0.887788, 0.774882, 0.303139] 0.030239\n",
      "7 [0.887788, 0.774883, 0.303139] 0.030237\n",
      "8 [0.887788, 0.774883, 0.303139] 0.030236\n",
      "9 [0.887788, 0.774884, 0.303139] 0.030235\n",
      "10 [0.887788, 0.774884, 0.303139] 0.030234\n",
      "11 [0.887788, 0.774885, 0.303139] 0.030232\n",
      "12 [0.887788, 0.774885, 0.303139] 0.030231\n",
      "13 [0.887788, 0.774886, 0.303139] 0.030230\n",
      "14 [0.887788, 0.774886, 0.303139] 0.030229\n",
      "15 [0.887788, 0.774887, 0.303139] 0.030227\n",
      "16 [0.887788, 0.774887, 0.303139] 0.030226\n",
      "17 [0.887788, 0.774888, 0.303139] 0.030225\n",
      "18 [0.887788, 0.774888, 0.303139] 0.030223\n",
      "19 [0.887788, 0.774889, 0.303139] 0.030222\n",
      "20 [0.887788, 0.774889, 0.303139] 0.030221\n",
      "21 [0.887788, 0.774890, 0.303139] 0.030220\n",
      "22 [0.887788, 0.774890, 0.303139] 0.030218\n",
      "23 [0.887788, 0.774891, 0.303139] 0.030217\n",
      "24 [0.887788, 0.774891, 0.303139] 0.030216\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "from functools import partial\n",
    "import torch\n",
    "from torch import optim\n",
    "# I got this example from https://discuss.pytorch.org/t/there-are-a-leverberg-marquardt-like-optimizer-for-pytorch/68055/3\n",
    "\n",
    "def rosenbrock(x):\n",
    "    return (1 - x[0])**2 + 100*(x[1] - x[0]**2)**2\n",
    "\n",
    "a = torch.tensor([0., 0., 0.]) + torch.rand(3)\n",
    "a_init = [float(a[0]), float(a[1]), float(a[2])]\n",
    "a.requires_grad_()\n",
    "loss = partial(rosenbrock, a)\n",
    "\n",
    "\n",
    "def print_iter(i, a, loss):\n",
    "    print(f'{i} [{a[0]:.6f}, {a[1]:.6f}, {a[2]:.6f}] {loss:.6f}')\n",
    "\n",
    "\n",
    "opt = optim.LBFGS([a], line_search_fn='strong_wolfe')\n",
    "print_iter(0, a, loss())\n",
    "torch_loss_interation = []\n",
    "for i in range(24):\n",
    "    opt.zero_grad()\n",
    "    loss().backward()\n",
    "    opt.step(loss)\n",
    "    torch_loss_interation.append(loss())\n",
    "    print_iter(i+1, a, loss())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might (or might not) already know I'm a Firedrake developer‚Äîyep, that's me! So, naturally, I will be exploring the `firedrake.adjoint` module. This **powerful** tool combines Firedrake's automatic differentiation capabilities with SciPy's optimization algorithms, making it perfect for tackling PDE-constrained optimization problems. üöÄ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 1.91 ¬µs\n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =            3     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  8.33590D-01    |proj g|=  1.51835D+01\n",
      "\n",
      "At iterate    1    f=  3.83680D-01    |proj g|=  6.05086D+00\n",
      "\n",
      "At iterate    2    f=  3.16594D-01    |proj g|=  2.15273D+00\n",
      "\n",
      "At iterate    3    f=  3.08543D-01    |proj g|=  5.83912D-01\n",
      "\n",
      "At iterate    4    f=  3.02234D-01    |proj g|=  1.44090D+00\n",
      "\n",
      "At iterate    5    f=  2.82629D-01    |proj g|=  2.53275D+00\n",
      "\n",
      "At iterate    6    f=  1.65780D-01    |proj g|=  4.13653D+00\n",
      "\n",
      "At iterate    7    f=  1.24142D-01    |proj g|=  5.51635D+00\n",
      "\n",
      "At iterate    8    f=  9.22824D-02    |proj g|=  1.71032D+00\n",
      "\n",
      "At iterate    9    f=  7.94258D-02    |proj g|=  3.51275D+00\n",
      "\n",
      "At iterate   10    f=  5.82256D-02    |proj g|=  3.39924D+00\n",
      "\n",
      "At iterate   11    f=  2.89178D-02    |proj g|=  3.32214D+00\n",
      "\n",
      "At iterate   12    f=  1.29408D-02    |proj g|=  2.61276D+00\n",
      "\n",
      "At iterate   13    f=  5.07394D-03    |proj g|=  1.28306D+00\n",
      "\n",
      "At iterat"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e   14    f=  2.45635D-03    |proj g|=  1.07109D+00\n",
      "\n",
      "At iterate   15    f=  1.17907D-03    |proj g|=  6.44250D-01\n",
      "\n",
      "At iterate   16    f=  4.95791D-04    |proj g|=  5.42251D-01\n",
      "\n",
      "At iterate   17    f=  6.54252D-05    |proj g|=  2.92141D-01\n",
      "\n",
      "At iterate   18    f=  5.32572D-06    |proj g|=  4.25161D-02\n",
      "\n",
      "At iterate   19    f=  6.93677D-07    |proj g|=  1.64864D-02\n",
      "\n",
      "At iterate   20    f=  2.75169D-10    |proj g|=  3.44461D-04\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "    3     20     26      1     0     0   3.445D-04   2.752D-10\n",
      "  F =   2.7516883153999110E-010\n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"PETSC_ARCH\"] = \"minimal_petsc\"\n",
    "os.environ[\"PETSC_DIR\"] = \"/Users/ddolci/tes_fire_install/petsc\"\n",
    "from firedrake import *\n",
    "from firedrake.adjoint import *\n",
    "continue_annotation()\n",
    "\n",
    "def test_optimisation_constant_control():\n",
    "    \"\"\"This tests a list of controls in a minimisation (through scipy L-BFGS-B)\"\"\"\n",
    "    mesh = UnitSquareMesh(1, 1)\n",
    "    R = FunctionSpace(mesh, \"R\", 0)\n",
    "\n",
    "    n = 3\n",
    "    x = [Function(R, val=float(a_init[i])) for i in range(n)]\n",
    "    c = [Control(xi) for xi in x]\n",
    "    \n",
    "    f = sum(100*(x[i+1] - x[i]**2)**2 + (1 - x[i])**2 for i in range(n-1))\n",
    "\n",
    "    J = assemble(f * dx(domain=mesh))\n",
    "    rf = ReducedFunctional(J, c)\n",
    "    # minimize is a method from pyadjoint.optimization\n",
    "    result = minimize(rf, method=\"L-BFGS-B\", options={\"disp\": True, \"maxiter\": 20})\n",
    "    return [float(result[i]) for i in range(n)]\n",
    "result = test_optimisation_constant_control()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "firedrake.adjoint result [0.9999936706742869, 0.9999874921565328, 0.9999758584787991]\n",
      "torch result [0.8877875208854675, 0.7748910784721375, 0.3031386137008667]\n"
     ]
    }
   ],
   "source": [
    "print(\"firedrake.adjoint result\", result)\n",
    "print(\"torch result\", [float(a[0]), float(a[1]), float(a[2])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we have a good start with two high-level algorithms! Let us compare their results. It seems like `firedrake.adjoint` combined with SciPy's optimization algorithms is more accurate than `torch` algorithms. But I'm not entirely sure if this comparison is fair... ü§î\n",
    "\n",
    " PyTorch. But I'm not entirely sure if this comparison is fair... ü§î\n",
    "\n",
    "Next up, I will explore the `torch.optim`. Stay in loop for more updates! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you want to try to use Firedrake? Here is the [installation guide](https://www.firedrakeproject.org/download.html). üåü\n",
    "\n",
    "It requires some time to install, but it's worth it! You can also use a Docker image with Firedrake pre-installed. üòâ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
